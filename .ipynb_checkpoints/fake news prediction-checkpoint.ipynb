{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1e134-6ee0-45d5-b588-c91417c2f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dependencies\n",
    "# The re dependency is a regular expression, used for searching for text in a document or paragraph\n",
    "# nltk - natural language toolkit\n",
    "# Stemming removes the suffix and prefix of a word, and returns the base of the word\n",
    "# TfidfVectorizer converts text into feature vectors (numbers)\n",
    "# stopwords are the words that do not add too much value to the text, eg. articles, where at, etc.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac459e1-cad7-4626-9282-878b01b1583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the nltk package \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c96a7-c919-4644-b520-fcf41edf62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words that do not add too much to a sentence, stopwords in english\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b070e8-6f73-4602-a134-2646a9470aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "\n",
    "news_data = pd.read_csv('./train.csv/train.csv')\n",
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5461df-8133-4aa9-a306-2d3a945f693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82798daf-910a-4b88-b784-ce8b7852191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the total number of labels\n",
    "news_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790a796-f485-4671-81f8-42bcf2d49c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the total number of titles\n",
    "# news_data['title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9964b5-53f0-4b02-9746-ffd9c92d3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the total number of missing values in the dataset\n",
    "news_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a87576-bf81-4e12-9cfe-c3ee24e03e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the null values with empty strings\n",
    "\n",
    "news_data = news_data.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315c8f4-9253-49c2-aa1f-05e091bf5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the missing data has been replaces with empty strings\n",
    "news_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad8468-c04a-4efe-a037-c93aecf80cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the author and title, they are the ones that will be used in the data analysis.......text cannot be used because they are too large\n",
    "# We are creating a new column called content, by combining author and title, but we leave a space between the two using the empty strings in between\n",
    "\n",
    "news_data['content'] = news_data['author'] +' '+ news_data['title']\n",
    "news_data['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2def36-bd2a-4da3-8246-93146b6ff3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb25f2-1749-4223-8c20-eb1dff172dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf98a96-03cf-4077-a70a-691b557dc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inputs and outputs(labels)\n",
    "\n",
    "X = news_data.drop(columns='label', axis=1)\n",
    "y = news_data['label']\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7997d0-b0e1-426e-ae62-c0c97d12972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ab5f8-5a26-4bc2-8af9-ebd0ba72a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming procedure - extracting the base form of words by removing the prefix and suffixes using the stopwords\n",
    "# example actor, actress, acting --> act\n",
    "# importing the porter stemmer module\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514659f-f465-419b-bbb2-683007f22537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function for the stemming\n",
    "# Content is the value to be passed into the function\n",
    "# The first line is the re.sub\n",
    "# The re.sub, re, is a regular expression that searches through text, and the .sub method kind of filters the the words and returns only alphabets, both small and capital.\n",
    "# It excludes all other characters like ,. numbers and returns alphabets alone.\n",
    "# The operation will be done on the content that has been passed over there.\n",
    "# The .lower function, converts all the alphabets to small letters\n",
    "# The split function converts all the letters to a list \n",
    "\n",
    "def stemming (content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6a9b3-c82e-46b5-a682-c6ff46e250c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to our content column, and returning a new content column with the stemmed words\n",
    "\n",
    "news_data['content'] = news_data['content'].apply(stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e01d5-05c3-4563-bc8c-179057a1afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055efad-31e4-453c-b168-dd75914b37ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
